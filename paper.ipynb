{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/hl/w24rw40s2ysg4f5hvvkbv3w80000gn/T/ipykernel_38972/553137385.py\", line 23, in <module>\n",
      "    from transformers import (\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/transformers/__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/transformers/utils/generic.py\", line 53, in <module>\n",
      "    import torch  # noqa: F401\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/I509335/projects/researches/de_proj/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "INFO:__main__:Loading data from 29 files for temporal analysis\n",
      "INFO:__main__:Loaded US_RepublicanParty_2016.csv: 1887 samples\n",
      "INFO:__main__:Loaded US_DemocraticParty_2020.csv: 2226 samples\n",
      "INFO:__main__:Loaded US_DemocraticParty_2008.csv: 1 samples\n",
      "WARNING:__main__:Unknown party 'SDP' in DE_SDPParty_2013.csv\n",
      "INFO:__main__:Loaded DE_CDUParty_2005.csv: 690 samples\n",
      "INFO:__main__:Loaded DE_CDUParty_2013.csv: 2525 samples\n",
      "WARNING:__main__:Unknown party 'SDP' in DE_SDPParty_2005.csv\n",
      "INFO:__main__:Loaded US_RepublicanParty_2000.csv: 1 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Political Semantic Drift Analysis\n",
      "Analyzing how political terms shift ideological meaning over time\n",
      "======================================================================\n",
      "📂 Loading political text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded DE_CDUParty_2002.csv: 1184 samples\n",
      "INFO:__main__:Loaded US_RepublicanParty_2004.csv: 1756 samples\n",
      "INFO:__main__:Loaded US_RepublicanParty_2011.csv: 1521 samples\n",
      "INFO:__main__:Loaded DE_CDUParty_2017.csv: 1195 samples\n",
      "WARNING:__main__:Unknown party 'SDP' in DE_SDPParty_2017.csv\n",
      "WARNING:__main__:Unknown party 'SDP' in DE_SDPParty_2002.csv\n",
      "INFO:__main__:Loaded DE_AFDParty_2021.csv: 1546 samples\n",
      "WARNING:__main__:Unknown party 'SDP' in DE_SDPParty_1998.csv\n",
      "INFO:__main__:Loaded DE_CDUParty_1998.csv: 504 samples\n",
      "INFO:__main__:Loaded DE_AFDParty_2017.csv: 964 samples\n",
      "INFO:__main__:Loaded DE_AFDParty_2013.csv: 67 samples\n",
      "INFO:__main__:Loaded US_DemocraticParty_2000.csv: 1 samples\n",
      "INFO:__main__:Loaded US_RepublicanParty_2020.csv: 1887 samples\n",
      "INFO:__main__:Loaded US_RepublicanParty_2008.csv: 983 samples\n",
      "INFO:__main__:Loaded US_DemocraticParty_2016.csv: 1281 samples\n",
      "WARNING:__main__:Unknown party 'SDP' in DE_SDPParty_2021.csv\n",
      "WARNING:__main__:Unknown party 'SDP' in DE_SDPParty_2009.csv\n",
      "INFO:__main__:Loaded US_DemocraticParty_2012.csv: 1227 samples\n",
      "INFO:__main__:Loaded US_DemocraticParty_2004.csv: 823 samples\n",
      "INFO:__main__:Loaded DE_CDUParty_2009.csv: 1711 samples\n",
      "INFO:__main__:Loaded DE_CDUParty_2021.csv: 2573 samples\n",
      "INFO:__main__:Total loaded: 26553 samples across 14 years\n",
      "INFO:__main__:Valid period-ideology combinations:\n",
      "INFO:__main__:  2000-2005 + LEFT: 824 samples\n",
      "INFO:__main__:  2000-2005 + RIGHT: 4135 samples\n",
      "INFO:__main__:  2006-2010 + RIGHT: 2694 samples\n",
      "INFO:__main__:  2011-2015 + LEFT: 1227 samples\n",
      "INFO:__main__:  2011-2015 + RIGHT: 4113 samples\n",
      "INFO:__main__:  2016-2022 + LEFT: 3507 samples\n",
      "INFO:__main__:  2016-2022 + RIGHT: 10052 samples\n",
      "INFO:__main__:Starting Political Semantic Drift Analysis\n",
      "INFO:__main__:Model types: ['baseline', 'roberta']\n",
      "INFO:__main__:Time periods: ['2000-2005', '2006-2010', '2011-2015', '2016-2022']\n",
      "INFO:__main__:\n",
      "=== Training BASELINE models ===\n",
      "INFO:__main__:Training baseline for period 2000-2005: 4959 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 4135, 'LEFT': 824}\n",
      "INFO:__main__:Training baseline model on 4959 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 26553 samples across 4 periods\n",
      "   Periods: ['2000-2005', '2006-2010', '2011-2015', '2016-2022']\n",
      "   Ideologies: ['LEFT', 'RIGHT']\n",
      "\n",
      "📊 Data distribution by period and ideology:\n",
      "ideology   LEFT  RIGHT\n",
      "period                \n",
      "2000-2005   824   4135\n",
      "2006-2010     1   2694\n",
      "2011-2015  1227   4113\n",
      "2016-2022  3507  10052\n",
      "\n",
      "🚀 Initializing semantic drift experiment...\n",
      "📋 Available models: ['baseline', 'roberta']\n",
      "⚡ Running semantic drift analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training class distribution: {np.str_('LEFT'): np.int64(824), np.str_('RIGHT'): np.int64(4135)}\n",
      "INFO:__main__:✅ Successfully trained baseline for 2000-2005\n",
      "INFO:__main__:Training baseline for period 2006-2010: 2695 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 2694, 'LEFT': 1}\n",
      "INFO:__main__:Training baseline model on 2695 samples\n",
      "INFO:__main__:Training class distribution: {np.str_('LEFT'): np.int64(1), np.str_('RIGHT'): np.int64(2694)}\n",
      "INFO:__main__:✅ Successfully trained baseline for 2006-2010\n",
      "INFO:__main__:Training baseline for period 2011-2015: 5340 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 4113, 'LEFT': 1227}\n",
      "INFO:__main__:Training baseline model on 5340 samples\n",
      "INFO:__main__:Training class distribution: {np.str_('LEFT'): np.int64(1227), np.str_('RIGHT'): np.int64(4113)}\n",
      "INFO:__main__:✅ Successfully trained baseline for 2011-2015\n",
      "INFO:__main__:Training baseline for period 2016-2022: 13559 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 10052, 'LEFT': 3507}\n",
      "INFO:__main__:Training baseline model on 13559 samples\n",
      "INFO:__main__:Training class distribution: {np.str_('LEFT'): np.int64(3507), np.str_('RIGHT'): np.int64(10052)}\n",
      "INFO:__main__:✅ Successfully trained baseline for 2016-2022\n",
      "INFO:__main__:\n",
      "=== Training ROBERTA models ===\n",
      "INFO:__main__:Training roberta for period 2000-2005: 4959 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 4135, 'LEFT': 824}\n",
      "INFO:__main__:Initializing roberta-base on cpu\n",
      "INFO:__main__:Training roberta-base on 4959 samples\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ERROR:__main__:❌ Failed to train roberta for 2000-2005: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`\n",
      "INFO:__main__:Training roberta for period 2006-2010: 2695 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 2694, 'LEFT': 1}\n",
      "INFO:__main__:Initializing roberta-base on cpu\n",
      "INFO:__main__:Training roberta-base on 2695 samples\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ERROR:__main__:❌ Failed to train roberta for 2006-2010: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`\n",
      "INFO:__main__:Training roberta for period 2011-2015: 5340 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 4113, 'LEFT': 1227}\n",
      "INFO:__main__:Initializing roberta-base on cpu\n",
      "INFO:__main__:Training roberta-base on 5340 samples\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ERROR:__main__:❌ Failed to train roberta for 2011-2015: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`\n",
      "INFO:__main__:Training roberta for period 2016-2022: 13559 samples\n",
      "INFO:__main__:  Class distribution: {'RIGHT': 10052, 'LEFT': 3507}\n",
      "INFO:__main__:Initializing roberta-base on cpu\n",
      "INFO:__main__:Training roberta-base on 13559 samples\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ERROR:__main__:❌ Failed to train roberta for 2016-2022: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`\n",
      "INFO:__main__:\n",
      "=== Analyzing Semantic Shifts ===\n",
      "INFO:__main__:Analyzing shifts for baseline\n",
      "INFO:__main__:Analyzing semantic shifts across 4 periods\n",
      "INFO:__main__:Analyzing semantic trajectory for: forces\n",
      "INFO:__main__:Analyzing semantic trajectory for: policy\n",
      "INFO:__main__:Analyzing semantic trajectory for: renewable\n",
      "INFO:__main__:Analyzing semantic trajectory for: free\n",
      "INFO:__main__:Analyzing semantic trajectory for: nuclear\n",
      "INFO:__main__:Analyzing semantic trajectory for: oil\n",
      "INFO:__main__:Analyzing semantic trajectory for: welfare\n",
      "INFO:__main__:Analyzing semantic trajectory for: civil\n",
      "INFO:__main__:Analyzing semantic trajectory for: gdp\n",
      "INFO:__main__:Analyzing semantic trajectory for: progress\n",
      "INFO:__main__:Analyzing semantic trajectory for: court\n",
      "INFO:__main__:Analyzing semantic trajectory for: tradition\n",
      "INFO:__main__:Analyzing semantic trajectory for: federal\n",
      "INFO:__main__:Analyzing semantic trajectory for: terror\n",
      "INFO:__main__:Analyzing semantic trajectory for: social\n",
      "INFO:__main__:Analyzing semantic trajectory for: support\n",
      "INFO:__main__:Analyzing semantic trajectory for: border\n",
      "INFO:__main__:Analyzing semantic trajectory for: change\n",
      "INFO:__main__:Analyzing semantic trajectory for: environment\n",
      "INFO:__main__:Analyzing semantic trajectory for: innovation\n",
      "INFO:__main__:Analyzing semantic trajectory for: law\n",
      "INFO:__main__:Analyzing semantic trajectory for: growth\n",
      "INFO:__main__:Analyzing semantic trajectory for: justice\n",
      "INFO:__main__:Analyzing semantic trajectory for: defense\n",
      "INFO:__main__:Analyzing semantic trajectory for: student\n",
      "INFO:__main__:Analyzing semantic trajectory for: military\n",
      "INFO:__main__:Analyzing semantic trajectory for: green\n",
      "INFO:__main__:Analyzing semantic trajectory for: equality\n",
      "INFO:__main__:Analyzing semantic trajectory for: school\n",
      "INFO:__main__:Analyzing semantic trajectory for: safety\n",
      "INFO:__main__:Detecting semantic flips: 2000-2005 -> 2006-2010\n",
      "INFO:__main__:Found 59 significant semantic shifts\n",
      "INFO:__main__:Detecting semantic flips: 2006-2010 -> 2011-2015\n",
      "INFO:__main__:Found 64 significant semantic shifts\n",
      "INFO:__main__:Detecting semantic flips: 2011-2015 -> 2016-2022\n",
      "INFO:__main__:Found 57 significant semantic shifts\n",
      "WARNING:__main__:Not enough periods for roberta analysis\n",
      "INFO:__main__:Saved semantic drift results: political_semantic_drift_analysis/semantic_drift_analysis.json\n",
      "INFO:__main__:Creating semantic drift visualizations...\n",
      "INFO:__main__:Saved trajectory plot: political_semantic_drift_analysis/term_trajectories_baseline.png\n",
      "INFO:__main__:Saved shift heatmap: political_semantic_drift_analysis/shift_heatmap_baseline.png\n",
      "INFO:__main__:Saved direction flip plot: political_semantic_drift_analysis/direction_flips_baseline.png\n",
      "INFO:__main__:Generating research findings...\n",
      "INFO:__main__:Created readable summary: political_semantic_drift_analysis/RESEARCH_SUMMARY.txt\n",
      "INFO:__main__:Saved research findings: political_semantic_drift_analysis/research_findings.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POLITICAL SEMANTIC DRIFT ANALYSIS - KEY FINDINGS ===\n",
      "\n",
      "📊 EXPERIMENT OVERVIEW:\n",
      "   • Models trained: 4\n",
      "   • Time periods: 4\n",
      "   • Model architectures: baseline\n",
      "\n",
      "🔍 BASELINE MODEL FINDINGS:\n",
      "   Most semantically volatile terms:\n",
      "      • green: 0.489 volatility score\n",
      "      • oil: 0.331 volatility score\n",
      "      • equality: 0.325 volatility score\n",
      "      • federal: 0.265 volatility score\n",
      "      • renewable: 0.256 volatility score\n",
      "   Terms with ideological direction flips:\n",
      "      • green: flipped 3 times\n",
      "      • oversight: flipped 3 times\n",
      "      • liberty: flipped 2 times\n",
      "      • insurance: flipped 2 times\n",
      "      • spending: flipped 2 times\n",
      "   Trend patterns: {'rightward': 14, 'stable': 5, 'leftward': 11}\n",
      "   Total significant shifts: 180\n",
      "\n",
      "💡 RESEARCH IMPLICATIONS:\n",
      "   • Political language shows measurable semantic drift over time\n",
      "   • Terms like 'reform' and 'security' change ideological associations\n",
      "   • Multiple model architectures provide validation of findings\n",
      "   • Temporal evaluation crucial for political NLP model robustness\n",
      "\n",
      "✅ Political semantic drift analysis completed!\n",
      "📊 Results saved to: political_semantic_drift_analysis\n",
      "📄 Check RESEARCH_SUMMARY.txt for key findings\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional, Any, Union\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Core ML imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Transformer imports\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModelForSequenceClassification, \n",
    "        RobertaTokenizer, RobertaForSequenceClassification,\n",
    "        BertTokenizer, BertForSequenceClassification,\n",
    "        Trainer, TrainingArguments\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    warnings.warn(\"Transformers/PyTorch not available. Only baseline models will be used.\")\n",
    "\n",
    "# SHAP for interpretability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    warnings.warn(\"SHAP not available. Semantic drift analysis will be limited.\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class SemanticShift:\n",
    "    \"\"\"Data structure for tracking semantic shifts of political terms\"\"\"\n",
    "    term: str\n",
    "    old_period: str\n",
    "    new_period: str\n",
    "    old_ideology_weights: Dict[str, float]  # ideology -> weight\n",
    "    new_ideology_weights: Dict[str, float]\n",
    "    shift_magnitude: float\n",
    "    direction_flip: bool  # True if term switched from left->right or vice versa\n",
    "    statistical_significance: float\n",
    "\n",
    "@dataclass \n",
    "class PoliticalTermAnalysis:\n",
    "    \"\"\"Analysis results for political terms over time\"\"\"\n",
    "    term: str\n",
    "    periods: List[str]\n",
    "    ideology_trajectories: Dict[str, List[float]]  # ideology -> [weights over time]\n",
    "    volatility_score: float\n",
    "    trend_direction: str  # 'leftward', 'rightward', 'stable', 'volatile'\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"Data manager focused on temporal political text analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, folder_path: str):\n",
    "        self.folder = Path(folder_path)\n",
    "        if not self.folder.exists():\n",
    "            raise FileNotFoundError(f\"Data folder not found: {folder_path}\")\n",
    "            \n",
    "        # Enhanced ideology mapping for better analysis\n",
    "        self.ideology_map = {\n",
    "            \"CDU\": \"RIGHT\",      # Conservative \n",
    "            \"AFD\": \"RIGHT\",      # Far-right    \n",
    "            \"SPD\": \"LEFT\",       # Social Democratic\n",
    "            \"REPUBLICAN\": \"RIGHT\",\n",
    "            \"DEMOCRATIC\": \"LEFT\",\n",
    "            \"CONSERVATIVE\": \"RIGHT\"\n",
    "        }\n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load data with focus on temporal analysis\"\"\"\n",
    "        files = list(self.folder.glob(\"*.csv\"))\n",
    "        if not files:\n",
    "            raise ValueError(f\"No CSV files found in {self.folder}\")\n",
    "            \n",
    "        logger.info(f\"Loading data from {len(files)} files for temporal analysis\")\n",
    "        all_dfs = []\n",
    "        \n",
    "        for f in files:\n",
    "            try:\n",
    "                df_processed = self._process_file(f)\n",
    "                if df_processed is not None and len(df_processed) > 0:\n",
    "                    all_dfs.append(df_processed)\n",
    "                    logger.info(f\"Loaded {f.name}: {len(df_processed)} samples\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {f.name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_dfs:\n",
    "            raise ValueError(\"No valid data files could be processed\")\n",
    "            \n",
    "        combined_data = pd.concat(all_dfs, ignore_index=True)\n",
    "        logger.info(f\"Total loaded: {len(combined_data)} samples across {len(combined_data['year'].unique())} years\")\n",
    "        \n",
    "        return self._prepare_for_temporal_analysis(combined_data)\n",
    "    \n",
    "    def _process_file(self, filepath: Path) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Process file with enhanced metadata extraction\"\"\"\n",
    "        try:\n",
    "            stem = filepath.stem\n",
    "            parts = stem.split('_')\n",
    "            \n",
    "            if len(parts) < 3:\n",
    "                return None\n",
    "                \n",
    "            country_code = parts[0].upper()\n",
    "            party_name = '_'.join(parts[1:-1]).upper()\n",
    "            party_name = party_name.replace(\"PARTY\", \"\")\n",
    "            year = int(parts[-1])\n",
    "            \n",
    "            if party_name not in self.ideology_map:\n",
    "                logger.warning(f\"Unknown party '{party_name}' in {filepath.name}\")\n",
    "                return None\n",
    "                \n",
    "            ideology = self.ideology_map[party_name]\n",
    "            \n",
    "            df_file = pd.read_csv(filepath, encoding='utf-8')\n",
    "            text_col = self._identify_text_column(df_file, country_code)\n",
    "            \n",
    "            if text_col is None:\n",
    "                return None\n",
    "                \n",
    "            result_df = pd.DataFrame({\n",
    "                'text': df_file[text_col],\n",
    "                'ideology': ideology,\n",
    "                'country': country_code,\n",
    "                'party': party_name,\n",
    "                'year': year,\n",
    "                'period': self._assign_period(year)  # Group years into periods\n",
    "            })\n",
    "            \n",
    "            return result_df[result_df['text'].str.len() >= 50]  # Filter very short texts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {filepath.name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _identify_text_column(self, df: pd.DataFrame, country_code: str) -> Optional[str]:\n",
    "        \"\"\"Identify text column\"\"\"\n",
    "        if country_code == 'DE' and 'text_en' in df.columns:\n",
    "            return 'text_en'\n",
    "        elif 'text' in df.columns:\n",
    "            return 'text'\n",
    "        else:\n",
    "            text_cols = [col for col in df.columns if 'text' in col.lower()]\n",
    "            return text_cols[0] if text_cols else None\n",
    "    \n",
    "    def _assign_period(self, year: int) -> str:\n",
    "        \"\"\"Assign years to periods for temporal analysis\"\"\"\n",
    "        if year <= 2005:\n",
    "            return \"2000-2005\"\n",
    "        elif year <= 2010:\n",
    "            return \"2006-2010\"\n",
    "        elif year <= 2015:\n",
    "            return \"2011-2015\"\n",
    "        else:\n",
    "            return \"2016-2022\"\n",
    "    \n",
    "    def _prepare_for_temporal_analysis(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare data specifically for temporal semantic analysis\"\"\"\n",
    "        # Ensure sufficient data per period and ideology\n",
    "        period_ideology_counts = data.groupby(['period', 'ideology']).size().reset_index(name='count')\n",
    "        \n",
    "        # Filter periods/ideologies with insufficient data\n",
    "        min_samples = 50\n",
    "        valid_combinations = period_ideology_counts[period_ideology_counts['count'] >= min_samples]\n",
    "        \n",
    "        logger.info(f\"Valid period-ideology combinations:\")\n",
    "        for _, row in valid_combinations.iterrows():\n",
    "            logger.info(f\"  {row['period']} + {row['ideology']}: {row['count']} samples\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "class PoliticalTermExtractor:\n",
    "    \"\"\"Extract and track political terms across time periods\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Comprehensive political vocabulary\n",
    "        self.political_terms = {\n",
    "            # Economic terms\n",
    "            'economy': ['economy', 'economic', 'economics', 'gdp', 'growth'],\n",
    "            'tax': ['tax', 'taxes', 'taxation', 'revenue', 'fiscal'],\n",
    "            'budget': ['budget', 'deficit', 'surplus', 'spending', 'debt'],\n",
    "            'reform': ['reform', 'reforms', 'reforming', 'restructure', 'change'],\n",
    "            \n",
    "            # Security & Defense\n",
    "            'security': ['security', 'secure', 'safety', 'protection', 'defend'],\n",
    "            'military': ['military', 'defense', 'defence', 'army', 'forces'],\n",
    "            'border': ['border', 'borders', 'immigration', 'migrant', 'refugee'],\n",
    "            'terrorism': ['terrorism', 'terrorist', 'terror', 'extremism'],\n",
    "            \n",
    "            # Social Issues\n",
    "            'healthcare': ['healthcare', 'health', 'medical', 'medicare', 'insurance'],\n",
    "            'education': ['education', 'school', 'schools', 'university', 'student'],\n",
    "            'welfare': ['welfare', 'social', 'benefits', 'assistance', 'support'],\n",
    "            'rights': ['rights', 'equality', 'discrimination', 'civil', 'freedom'],\n",
    "            \n",
    "            # Environmental\n",
    "            'environment': ['environment', 'climate', 'green', 'carbon', 'emission'],\n",
    "            'energy': ['energy', 'renewable', 'oil', 'gas', 'nuclear'],\n",
    "            \n",
    "            # Governance\n",
    "            'government': ['government', 'federal', 'state', 'administration', 'policy'],\n",
    "            'regulation': ['regulation', 'regulate', 'deregulation', 'oversight'],\n",
    "            'justice': ['justice', 'court', 'legal', 'law', 'judicial'],\n",
    "            \n",
    "            # Values & Identity\n",
    "            'freedom': ['freedom', 'liberty', 'free', 'independence'],\n",
    "            'tradition': ['tradition', 'traditional', 'conservative', 'values'],\n",
    "            'progress': ['progress', 'progressive', 'modern', 'innovation'],\n",
    "            'family': ['family', 'families', 'marriage', 'children']\n",
    "        }\n",
    "        \n",
    "        # Flatten for easy lookup\n",
    "        self.all_political_terms = set()\n",
    "        for category, terms in self.political_terms.items():\n",
    "            self.all_political_terms.update(terms)\n",
    "    \n",
    "    def extract_political_context(self, text: str, term: str, window_size: int = 5) -> List[str]:\n",
    "        \"\"\"Extract context words around political terms\"\"\"\n",
    "        words = text.lower().split()\n",
    "        contexts = []\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            if term in word:\n",
    "                start = max(0, i - window_size)\n",
    "                end = min(len(words), i + window_size + 1)\n",
    "                context = ' '.join(words[start:end])\n",
    "                contexts.append(context)\n",
    "        \n",
    "        return contexts\n",
    "\n",
    "class ModelWrapper(ABC):\n",
    "    \"\"\"Abstract model wrapper for political text analysis\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, texts: List[str], labels: List[str]) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, texts: List[str]) -> Tuple[List[str], List[float]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_political_term_weights(self, terms: List[str]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get ideology weights for specific political terms\"\"\"\n",
    "        pass\n",
    "\n",
    "class BaselineModel(ModelWrapper):\n",
    "    \"\"\"Baseline TF-IDF + Logistic Regression model\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features: int = 15000):\n",
    "        self.pipeline = Pipeline([\n",
    "            (\"tfidf\", TfidfVectorizer(\n",
    "                max_features=max_features,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=3,\n",
    "                max_df=0.9,\n",
    "                stop_words='english',\n",
    "                lowercase=True\n",
    "            )),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=42,\n",
    "                C=1.0\n",
    "            ))\n",
    "        ])\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_trained = False\n",
    "\n",
    "    def train(self, texts: List[str], labels: List[str]) -> None:\n",
    "        logger.info(f\"Training baseline model on {len(texts)} samples\")\n",
    "        y = self.label_encoder.fit_transform(labels)\n",
    "        self.pipeline.fit(texts, y)\n",
    "        self.is_trained = True\n",
    "        \n",
    "        # Log class distribution\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        class_dist = dict(zip(self.label_encoder.inverse_transform(unique), counts))\n",
    "        logger.info(f\"Training class distribution: {class_dist}\")\n",
    "\n",
    "    def predict(self, texts: List[str]) -> Tuple[List[str], List[float]]:\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model not trained\")\n",
    "            \n",
    "        proba = self.pipeline.predict_proba(texts)\n",
    "        pred_indices = proba.argmax(axis=1)\n",
    "        labels = self.label_encoder.inverse_transform(pred_indices)\n",
    "        confidence = proba.max(axis=1)\n",
    "        \n",
    "        return labels.tolist(), confidence.tolist()\n",
    "\n",
    "    def get_political_term_weights(self, terms: List[str]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Extract weights for political terms across ideologies\"\"\"\n",
    "        if not self.is_trained:\n",
    "            return {}\n",
    "            \n",
    "        vectorizer = self.pipeline.named_steps[\"tfidf\"]\n",
    "        clf = self.pipeline.named_steps[\"clf\"]\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Create mapping from feature names to indices\n",
    "        feature_to_idx = {name: idx for idx, name in enumerate(feature_names)}\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for term in terms:\n",
    "            term_weights = {}\n",
    "            \n",
    "            # Find features containing this term\n",
    "            matching_features = [f for f in feature_names if term.lower() in f.lower()]\n",
    "            \n",
    "            if not matching_features:\n",
    "                continue\n",
    "            \n",
    "            # Get average weight for this term across all matching features\n",
    "            for i, ideology in enumerate(self.label_encoder.classes_):\n",
    "                if len(self.label_encoder.classes_) == 2:\n",
    "                    # Binary classification\n",
    "                    coef = clf.coef_[0] if i == 1 else -clf.coef_[0]\n",
    "                else:\n",
    "                    # Multi-class\n",
    "                    coef = clf.coef_[i]\n",
    "                \n",
    "                # Average weight across all features containing the term\n",
    "                term_weight = np.mean([coef[feature_to_idx[f]] for f in matching_features])\n",
    "                term_weights[ideology] = float(term_weight)\n",
    "            \n",
    "            results[term] = term_weights\n",
    "        \n",
    "        return results\n",
    "\n",
    "class TransformerModel(ModelWrapper):\n",
    "    \"\"\"BERT/RoBERTa model wrapper with interpretability - FIXED VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"roberta-base\"):\n",
    "        if not TRANSFORMERS_AVAILABLE:\n",
    "            raise ImportError(\"Transformers library not available\")\n",
    "            \n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_trained = False\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        logger.info(f\"Initializing {model_name} on {self.device}\")\n",
    "\n",
    "    def train(self, texts: List[str], labels: List[str]) -> None:\n",
    "        logger.info(f\"Training {self.model_name} on {len(texts)} samples\")\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        if 'roberta' in self.model_name:\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "                self.model_name, num_labels=len(set(labels))\n",
    "            )\n",
    "        else:  # BERT\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = BertForSequenceClassification.from_pretrained(\n",
    "                self.model_name, num_labels=len(set(labels))\n",
    "            )\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Prepare data\n",
    "        y = self.label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Tokenize texts\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create dataset\n",
    "        class PoliticalDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, encodings, labels):\n",
    "                self.encodings = encodings\n",
    "                self.labels = labels\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "                return item\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "        \n",
    "        dataset = PoliticalDataset(encodings, y)\n",
    "        \n",
    "        # FIXED: Updated training arguments with version compatibility\n",
    "        try:\n",
    "            # Try newer transformers version parameter name\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir='./temp_results',\n",
    "                num_train_epochs=2,  # Reduced for faster training\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=16,\n",
    "                warmup_steps=100,  # Reduced warmup\n",
    "                weight_decay=0.01,\n",
    "                logging_dir='./temp_logs',\n",
    "                logging_steps=100,\n",
    "                save_strategy=\"no\",\n",
    "                eval_strategy=\"no\",  # New parameter name\n",
    "                report_to=[],\n",
    "                remove_unused_columns=True,\n",
    "                dataloader_pin_memory=False,\n",
    "                disable_tqdm=False\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback for older transformers versions\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir='./temp_results',\n",
    "                num_train_epochs=2,\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=16,\n",
    "                warmup_steps=100,\n",
    "                weight_decay=0.01,\n",
    "                logging_dir='./temp_logs',\n",
    "                logging_steps=100,\n",
    "                save_strategy=\"no\",\n",
    "                evaluation_strategy=\"no\",  # Old parameter name\n",
    "                report_to=[],\n",
    "                remove_unused_columns=True,\n",
    "                dataloader_pin_memory=False,\n",
    "                disable_tqdm=False\n",
    "            )\n",
    "        \n",
    "        # Train\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            trainer.train()\n",
    "            self.is_trained = True\n",
    "            logger.info(f\"✅ {self.model_name} training completed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, texts: List[str]) -> Tuple[List[str], List[float]]:\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model not trained\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        batch_size = 16\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            encodings = self.tokenizer(\n",
    "                batch_texts,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encodings)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                \n",
    "                batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "                batch_confs = torch.max(probs, dim=-1)[0].cpu().numpy()\n",
    "                \n",
    "                predictions.extend(batch_preds)\n",
    "                confidences.extend(batch_confs)\n",
    "        \n",
    "        # Convert predictions to labels\n",
    "        labels = self.label_encoder.inverse_transform(predictions)\n",
    "        \n",
    "        return labels.tolist(), confidences.tolist()\n",
    "\n",
    "    def get_political_term_weights(self, terms: List[str]) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Use simplified attention-based approach for political term importance\"\"\"\n",
    "        if not self.is_trained:\n",
    "            return {}\n",
    "        \n",
    "        # Simplified approach: analyze model's internal representations\n",
    "        results = {}\n",
    "        \n",
    "        for term in terms:\n",
    "            # Create simple test sentences with the term\n",
    "            test_sentences = [\n",
    "                f\"The {term} policy is important.\",\n",
    "                f\"We support {term} reform.\",\n",
    "                f\"The {term} issue needs attention.\"\n",
    "            ]\n",
    "            \n",
    "            term_weights = {}\n",
    "            \n",
    "            try:\n",
    "                predictions, confidences = self.predict(test_sentences)\n",
    "                \n",
    "                # Simple heuristic: if model consistently predicts one ideology\n",
    "                # for sentences containing this term, assign weight accordingly\n",
    "                ideology_counts = Counter(predictions)\n",
    "                total_predictions = len(predictions)\n",
    "                \n",
    "                for ideology in self.label_encoder.classes_:\n",
    "                    count = ideology_counts.get(ideology, 0)\n",
    "                    weight = (count / total_predictions - 0.5) * 2  # Normalize to [-1, 1]\n",
    "                    term_weights[ideology] = float(weight)\n",
    "                \n",
    "                results[term] = term_weights\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not get weights for term '{term}': {str(e)}\")\n",
    "                # Return zero weights as fallback\n",
    "                for ideology in self.label_encoder.classes_:\n",
    "                    term_weights[ideology] = 0.0\n",
    "                results[term] = term_weights\n",
    "        \n",
    "        return results\n",
    "\n",
    "class SemanticShiftAnalyzer:\n",
    "    \"\"\"Core analyzer for detecting semantic shifts in political language\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.term_extractor = PoliticalTermExtractor()\n",
    "        self.shift_threshold = 0.1  # Minimum weight change to consider significant\n",
    "        \n",
    "    def analyze_temporal_shifts(self, \n",
    "                              models_by_period: Dict[str, ModelWrapper],\n",
    "                              periods: List[str]) -> Dict[str, PoliticalTermAnalysis]:\n",
    "        \"\"\"Analyze how political terms shift meaning across time periods\"\"\"\n",
    "        \n",
    "        logger.info(f\"Analyzing semantic shifts across {len(periods)} periods\")\n",
    "        \n",
    "        # Get political terms to analyze\n",
    "        terms_to_analyze = list(self.term_extractor.all_political_terms)[:30]  # Top 30 terms\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for term in terms_to_analyze:\n",
    "            logger.info(f\"Analyzing semantic trajectory for: {term}\")\n",
    "            \n",
    "            ideology_trajectories = defaultdict(list)\n",
    "            \n",
    "            # Get term weights for each period\n",
    "            for period in periods:\n",
    "                if period not in models_by_period:\n",
    "                    continue\n",
    "                    \n",
    "                model = models_by_period[period]\n",
    "                term_weights = model.get_political_term_weights([term])\n",
    "                \n",
    "                if term in term_weights:\n",
    "                    for ideology, weight in term_weights[term].items():\n",
    "                        ideology_trajectories[ideology].append(weight)\n",
    "                else:\n",
    "                    # Term not found in this period\n",
    "                    for ideology in ['LEFT', 'RIGHT']:  # Focus on main ideologies\n",
    "                        ideology_trajectories[ideology].append(0.0)\n",
    "            \n",
    "            # Calculate trajectory statistics\n",
    "            volatility = self._calculate_volatility(ideology_trajectories)\n",
    "            trend = self._determine_trend(ideology_trajectories)\n",
    "            \n",
    "            results[term] = PoliticalTermAnalysis(\n",
    "                term=term,\n",
    "                periods=periods,\n",
    "                ideology_trajectories=dict(ideology_trajectories),\n",
    "                volatility_score=volatility,\n",
    "                trend_direction=trend\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def detect_semantic_flips(self, \n",
    "                            old_model: ModelWrapper, \n",
    "                            new_model: ModelWrapper,\n",
    "                            old_period: str,\n",
    "                            new_period: str) -> List[SemanticShift]:\n",
    "        \"\"\"Detect terms that flipped ideological association\"\"\"\n",
    "        \n",
    "        logger.info(f\"Detecting semantic flips: {old_period} -> {new_period}\")\n",
    "        \n",
    "        terms_to_analyze = list(self.term_extractor.all_political_terms)\n",
    "        \n",
    "        old_weights = old_model.get_political_term_weights(terms_to_analyze)\n",
    "        new_weights = new_model.get_political_term_weights(terms_to_analyze)\n",
    "        \n",
    "        semantic_shifts = []\n",
    "        \n",
    "        for term in terms_to_analyze:\n",
    "            if term not in old_weights or term not in new_weights:\n",
    "                continue\n",
    "                \n",
    "            old_term_weights = old_weights[term]\n",
    "            new_term_weights = new_weights[term]\n",
    "            \n",
    "            # Calculate shift magnitude\n",
    "            shift_magnitude = self._calculate_shift_magnitude(old_term_weights, new_term_weights)\n",
    "            \n",
    "            # Check for ideological flip\n",
    "            direction_flip = self._detect_direction_flip(old_term_weights, new_term_weights)\n",
    "            \n",
    "            if shift_magnitude > self.shift_threshold or direction_flip:\n",
    "                shift = SemanticShift(\n",
    "                    term=term,\n",
    "                    old_period=old_period,\n",
    "                    new_period=new_period,\n",
    "                    old_ideology_weights=old_term_weights,\n",
    "                    new_ideology_weights=new_term_weights,\n",
    "                    shift_magnitude=shift_magnitude,\n",
    "                    direction_flip=direction_flip,\n",
    "                    statistical_significance=self._calculate_significance(\n",
    "                        old_term_weights, new_term_weights\n",
    "                    )\n",
    "                )\n",
    "                semantic_shifts.append(shift)\n",
    "        \n",
    "        # Sort by shift magnitude\n",
    "        semantic_shifts.sort(key=lambda x: x.shift_magnitude, reverse=True)\n",
    "        \n",
    "        logger.info(f\"Found {len(semantic_shifts)} significant semantic shifts\")\n",
    "        \n",
    "        return semantic_shifts\n",
    "    \n",
    "    def _calculate_volatility(self, trajectories: Dict[str, List[float]]) -> float:\n",
    "        \"\"\"Calculate volatility score for term across ideologies\"\"\"\n",
    "        all_changes = []\n",
    "        \n",
    "        for ideology, weights in trajectories.items():\n",
    "            if len(weights) > 1:\n",
    "                changes = np.abs(np.diff(weights))\n",
    "                all_changes.extend(changes)\n",
    "        \n",
    "        return float(np.mean(all_changes)) if all_changes else 0.0\n",
    "    \n",
    "    def _determine_trend(self, trajectories: Dict[str, List[float]]) -> str:\n",
    "        \"\"\"Determine overall trend direction\"\"\"\n",
    "        ideology_trends = {}\n",
    "        \n",
    "        for ideology, weights in trajectories.items():\n",
    "            if len(weights) > 1:\n",
    "                # Simple linear trend\n",
    "                x = np.arange(len(weights))\n",
    "                slope = np.polyfit(x, weights, 1)[0]\n",
    "                ideology_trends[ideology] = slope\n",
    "        \n",
    "        if not ideology_trends:\n",
    "            return 'stable'\n",
    "        \n",
    "        # Determine dominant trend\n",
    "        left_trend = ideology_trends.get('LEFT', 0)\n",
    "        right_trend = ideology_trends.get('RIGHT', 0)\n",
    "        \n",
    "        if abs(left_trend) > abs(right_trend) and abs(left_trend) > 0.01:\n",
    "            return 'leftward' if left_trend > 0 else 'rightward'\n",
    "        elif abs(right_trend) > 0.01:\n",
    "            return 'rightward' if right_trend > 0 else 'leftward'\n",
    "        else:\n",
    "            return 'stable'\n",
    "    \n",
    "    def _calculate_shift_magnitude(self, \n",
    "                                 old_weights: Dict[str, float], \n",
    "                                 new_weights: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate magnitude of semantic shift\"\"\"\n",
    "        shifts = []\n",
    "        \n",
    "        for ideology in old_weights:\n",
    "            if ideology in new_weights:\n",
    "                shift = abs(new_weights[ideology] - old_weights[ideology])\n",
    "                shifts.append(shift)\n",
    "        \n",
    "        return float(np.mean(shifts)) if shifts else 0.0\n",
    "    \n",
    "    def _detect_direction_flip(self, \n",
    "                             old_weights: Dict[str, float], \n",
    "                             new_weights: Dict[str, float]) -> bool:\n",
    "        \"\"\"Detect if term flipped ideological direction\"\"\"\n",
    "        \n",
    "        # Check if strongest association flipped\n",
    "        old_max_ideology = max(old_weights, key=old_weights.get)\n",
    "        new_max_ideology = max(new_weights, key=new_weights.get)\n",
    "        \n",
    "        # Simple flip detection: if strongest ideology changed and weights are significant\n",
    "        if (old_max_ideology != new_max_ideology and \n",
    "            abs(old_weights[old_max_ideology]) > 0.05 and \n",
    "            abs(new_weights[new_max_ideology]) > 0.05):\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _calculate_significance(self, \n",
    "                              old_weights: Dict[str, float], \n",
    "                              new_weights: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate statistical significance of shift (simplified)\"\"\"\n",
    "        # This is a placeholder - real implementation would use proper statistical tests\n",
    "        shift_magnitude = self._calculate_shift_magnitude(old_weights, new_weights)\n",
    "        return min(1.0, shift_magnitude * 10)  # Simple approximation\n",
    "\n",
    "class SemanticDriftExperiment:\n",
    "    \"\"\"Main experiment class for political semantic drift analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, output_dir: str = \"semantic_drift_results\"):\n",
    "        self.data = data\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.analyzer = SemanticShiftAnalyzer()\n",
    "        self.models_by_period = {}\n",
    "        \n",
    "        # Setup logging\n",
    "        log_file = self.output_dir / \"semantic_drift.log\"\n",
    "        handler = logging.FileHandler(log_file)\n",
    "        handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    def run_experiment(self, \n",
    "                      model_types: List[str] = [\"baseline\", \"roberta\"],\n",
    "                      min_samples_per_period: int = 100):\n",
    "        \"\"\"Run complete semantic drift experiment\"\"\"\n",
    "        \n",
    "        logger.info(\"Starting Political Semantic Drift Analysis\")\n",
    "        logger.info(f\"Model types: {model_types}\")\n",
    "        \n",
    "        periods = sorted(self.data['period'].unique())\n",
    "        logger.info(f\"Time periods: {periods}\")\n",
    "        \n",
    "        # Train models for each period and model type\n",
    "        for model_type in model_types:\n",
    "            logger.info(f\"\\n=== Training {model_type.upper()} models ===\")\n",
    "            self.models_by_period[model_type] = {}\n",
    "            \n",
    "            for period in periods:\n",
    "                period_data = self.data[self.data['period'] == period]\n",
    "                \n",
    "                if len(period_data) < min_samples_per_period:\n",
    "                    logger.warning(f\"Insufficient data for {period}: {len(period_data)} samples\")\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Training {model_type} for period {period}: {len(period_data)} samples\")\n",
    "                \n",
    "                # Check class balance\n",
    "                class_counts = period_data['ideology'].value_counts()\n",
    "                logger.info(f\"  Class distribution: {class_counts.to_dict()}\")\n",
    "                \n",
    "                if len(class_counts) < 2:\n",
    "                    logger.warning(f\"Only one class in {period}, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Initialize model\n",
    "                if model_type == \"baseline\":\n",
    "                    model = BaselineModel()\n",
    "                elif model_type == \"roberta\" and TRANSFORMERS_AVAILABLE:\n",
    "                    model = TransformerModel(\"roberta-base\")\n",
    "                elif model_type == \"bert\" and TRANSFORMERS_AVAILABLE:\n",
    "                    model = TransformerModel(\"bert-base-uncased\")\n",
    "                else:\n",
    "                    logger.warning(f\"Model type {model_type} not available, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Train model\n",
    "                texts = period_data['text'].tolist()\n",
    "                labels = period_data['ideology'].tolist()\n",
    "                \n",
    "                try:\n",
    "                    model.train(texts, labels)\n",
    "                    self.models_by_period[model_type][period] = model\n",
    "                    logger.info(f\"✅ Successfully trained {model_type} for {period}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"❌ Failed to train {model_type} for {period}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        # Analyze semantic shifts\n",
    "        self._analyze_and_save_results()\n",
    "    \n",
    "    def _analyze_and_save_results(self):\n",
    "        \"\"\"Analyze semantic shifts and save results\"\"\"\n",
    "        \n",
    "        logger.info(\"\\n=== Analyzing Semantic Shifts ===\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_type, period_models in self.models_by_period.items():\n",
    "            if len(period_models) < 2:\n",
    "                logger.warning(f\"Not enough periods for {model_type} analysis\")\n",
    "                continue\n",
    "                \n",
    "            logger.info(f\"Analyzing shifts for {model_type}\")\n",
    "            \n",
    "            periods = sorted(period_models.keys())\n",
    "            \n",
    "            # Temporal trajectory analysis\n",
    "            trajectory_analysis = self.analyzer.analyze_temporal_shifts(\n",
    "                period_models, periods\n",
    "            )\n",
    "            \n",
    "            # Pairwise shift detection\n",
    "            pairwise_shifts = {}\n",
    "            for i in range(len(periods) - 1):\n",
    "                old_period = periods[i]\n",
    "                new_period = periods[i + 1]\n",
    "                \n",
    "                shifts = self.analyzer.detect_semantic_flips(\n",
    "                    period_models[old_period],\n",
    "                    period_models[new_period], \n",
    "                    old_period,\n",
    "                    new_period\n",
    "                )\n",
    "                \n",
    "                pairwise_shifts[f\"{old_period}->{new_period}\"] = shifts\n",
    "            \n",
    "            results[model_type] = {\n",
    "                'trajectory_analysis': trajectory_analysis,\n",
    "                'pairwise_shifts': pairwise_shifts\n",
    "            }\n",
    "        \n",
    "        # Save results\n",
    "        self._save_semantic_drift_results(results)\n",
    "        self._create_visualizations(results)\n",
    "        self._generate_research_findings(results)\n",
    "    \n",
    "    def _save_semantic_drift_results(self, results: Dict):\n",
    "        \"\"\"Save detailed semantic drift results\"\"\"\n",
    "        \n",
    "        # Convert results to JSON-serializable format\n",
    "        json_results = {}\n",
    "        \n",
    "        for model_type, model_results in results.items():\n",
    "            json_results[model_type] = {\n",
    "                'trajectory_analysis': {},\n",
    "                'pairwise_shifts': {}\n",
    "            }\n",
    "            \n",
    "            # Convert trajectory analysis\n",
    "            for term, analysis in model_results['trajectory_analysis'].items():\n",
    "                json_results[model_type]['trajectory_analysis'][term] = {\n",
    "                    'term': analysis.term,\n",
    "                    'periods': analysis.periods,\n",
    "                    'ideology_trajectories': analysis.ideology_trajectories,\n",
    "                    'volatility_score': float(analysis.volatility_score),\n",
    "                    'trend_direction': analysis.trend_direction\n",
    "                }\n",
    "            \n",
    "            # Convert pairwise shifts\n",
    "            for period_pair, shifts in model_results['pairwise_shifts'].items():\n",
    "                json_results[model_type]['pairwise_shifts'][period_pair] = []\n",
    "                \n",
    "                for shift in shifts[:10]:  # Top 10 shifts per period pair\n",
    "                    json_results[model_type]['pairwise_shifts'][period_pair].append({\n",
    "                        'term': shift.term,\n",
    "                        'old_period': shift.old_period,\n",
    "                        'new_period': shift.new_period,\n",
    "                        'old_ideology_weights': shift.old_ideology_weights,\n",
    "                        'new_ideology_weights': shift.new_ideology_weights,\n",
    "                        'shift_magnitude': float(shift.shift_magnitude),\n",
    "                        'direction_flip': shift.direction_flip,\n",
    "                        'statistical_significance': float(shift.statistical_significance)\n",
    "                    })\n",
    "        \n",
    "        # Save to file\n",
    "        results_file = self.output_dir / \"semantic_drift_analysis.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Saved semantic drift results: {results_file}\")\n",
    "    \n",
    "    def _create_visualizations(self, results: Dict):\n",
    "        \"\"\"Create visualizations for semantic drift patterns\"\"\"\n",
    "        \n",
    "        logger.info(\"Creating semantic drift visualizations...\")\n",
    "        \n",
    "        for model_type, model_results in results.items():\n",
    "            \n",
    "            # 1. Political term trajectory plots\n",
    "            self._plot_term_trajectories(\n",
    "                model_results['trajectory_analysis'], \n",
    "                model_type\n",
    "            )\n",
    "            \n",
    "            # 2. Semantic shift heatmaps\n",
    "            self._plot_shift_heatmaps(\n",
    "                model_results['pairwise_shifts'],\n",
    "                model_type\n",
    "            )\n",
    "            \n",
    "            # 3. Direction flip analysis\n",
    "            self._plot_direction_flips(\n",
    "                model_results['pairwise_shifts'],\n",
    "                model_type\n",
    "            )\n",
    "    \n",
    "    def _plot_term_trajectories(self, trajectory_analysis: Dict, model_type: str):\n",
    "        \"\"\"Plot how political terms evolve across time periods\"\"\"\n",
    "        \n",
    "        # Select top 12 most volatile terms\n",
    "        top_terms = sorted(\n",
    "            trajectory_analysis.items(),\n",
    "            key=lambda x: x[1].volatility_score,\n",
    "            reverse=True\n",
    "        )[:12]\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        fig.suptitle(f'Political Term Trajectories - {model_type.upper()}', fontsize=16)\n",
    "        \n",
    "        for idx, (term, analysis) in enumerate(top_terms):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            periods = analysis.periods\n",
    "            x_pos = range(len(periods))\n",
    "            \n",
    "            # Plot trajectory for each ideology\n",
    "            for ideology, weights in analysis.ideology_trajectories.items():\n",
    "                if len(weights) == len(periods):\n",
    "                    ax.plot(x_pos, weights, 'o-', label=ideology, linewidth=2, markersize=6)\n",
    "            \n",
    "            ax.set_title(f'{term}\\n(volatility: {analysis.volatility_score:.3f})', fontsize=10)\n",
    "            ax.set_xlabel('Time Period')\n",
    "            ax.set_ylabel('Ideology Weight')\n",
    "            ax.set_xticks(x_pos)\n",
    "            ax.set_xticklabels(periods, rotation=45, ha='right', fontsize=8)\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(len(top_terms), len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plot_file = self.output_dir / f\"term_trajectories_{model_type}.png\"\n",
    "        plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Saved trajectory plot: {plot_file}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_shift_heatmaps(self, pairwise_shifts: Dict, model_type: str):\n",
    "        \"\"\"Create heatmaps showing semantic shift magnitudes\"\"\"\n",
    "        \n",
    "        # Collect all terms and their shifts\n",
    "        all_terms = set()\n",
    "        shift_data = {}\n",
    "        \n",
    "        for period_pair, shifts in pairwise_shifts.items():\n",
    "            shift_data[period_pair] = {}\n",
    "            for shift in shifts:\n",
    "                all_terms.add(shift.term)\n",
    "                shift_data[period_pair][shift.term] = shift.shift_magnitude\n",
    "        \n",
    "        # Create matrix for heatmap\n",
    "        terms_list = sorted(list(all_terms))[:20]  # Top 20 terms\n",
    "        period_pairs = list(shift_data.keys())\n",
    "        \n",
    "        if not terms_list or not period_pairs:\n",
    "            logger.warning(f\"No data for heatmap - {model_type}\")\n",
    "            return\n",
    "        \n",
    "        matrix = np.zeros((len(terms_list), len(period_pairs)))\n",
    "        \n",
    "        for j, period_pair in enumerate(period_pairs):\n",
    "            for i, term in enumerate(terms_list):\n",
    "                matrix[i, j] = shift_data[period_pair].get(term, 0)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(\n",
    "            matrix,\n",
    "            xticklabels=period_pairs,\n",
    "            yticklabels=terms_list,\n",
    "            annot=True,\n",
    "            fmt='.3f',\n",
    "            cmap='YlOrRd',\n",
    "            cbar_kws={'label': 'Shift Magnitude'}\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Semantic Shift Magnitudes - {model_type.upper()}')\n",
    "        plt.xlabel('Time Period Transitions')\n",
    "        plt.ylabel('Political Terms')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        heatmap_file = self.output_dir / f\"shift_heatmap_{model_type}.png\"\n",
    "        plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Saved shift heatmap: {heatmap_file}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_direction_flips(self, pairwise_shifts: Dict, model_type: str):\n",
    "        \"\"\"Visualize terms that flip ideological direction\"\"\"\n",
    "        \n",
    "        flip_data = []\n",
    "        \n",
    "        for period_pair, shifts in pairwise_shifts.items():\n",
    "            flips = [s for s in shifts if s.direction_flip]\n",
    "            \n",
    "            for shift in flips[:5]:  # Top 5 flips per period\n",
    "                flip_data.append({\n",
    "                    'term': shift.term,\n",
    "                    'period_transition': period_pair,\n",
    "                    'shift_magnitude': shift.shift_magnitude,\n",
    "                    'old_strongest': max(shift.old_ideology_weights, key=shift.old_ideology_weights.get),\n",
    "                    'new_strongest': max(shift.new_ideology_weights, key=shift.new_ideology_weights.get)\n",
    "                })\n",
    "        \n",
    "        if not flip_data:\n",
    "            logger.info(f\"No direction flips detected for {model_type}\")\n",
    "            return\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Plot 1: Flip magnitudes\n",
    "        flip_df = pd.DataFrame(flip_data)\n",
    "        \n",
    "        if len(flip_df) > 0:\n",
    "            flip_summary = flip_df.groupby('term')['shift_magnitude'].max().sort_values(ascending=False)[:15]\n",
    "            \n",
    "            ax1.barh(range(len(flip_summary)), flip_summary.values)\n",
    "            ax1.set_yticks(range(len(flip_summary)))\n",
    "            ax1.set_yticklabels(flip_summary.index)\n",
    "            ax1.set_xlabel('Maximum Shift Magnitude')\n",
    "            ax1.set_title('Terms with Ideological Direction Flips')\n",
    "            \n",
    "            # Plot 2: Flip transitions\n",
    "            flip_transitions = flip_df.groupby(['old_strongest', 'new_strongest']).size().reset_index(name='count')\n",
    "            \n",
    "            if len(flip_transitions) > 0:\n",
    "                transition_matrix = flip_transitions.pivot(\n",
    "                    index='old_strongest', \n",
    "                    columns='new_strongest', \n",
    "                    values='count'\n",
    "                ).fillna(0)\n",
    "                \n",
    "                sns.heatmap(\n",
    "                    transition_matrix,\n",
    "                    annot=True,\n",
    "                    fmt='g',\n",
    "                    cmap='Blues',\n",
    "                    ax=ax2,\n",
    "                    cbar_kws={'label': 'Number of Terms'}\n",
    "                )\n",
    "                ax2.set_title('Ideological Transition Patterns')\n",
    "                ax2.set_xlabel('New Strongest Association')\n",
    "                ax2.set_ylabel('Old Strongest Association')\n",
    "        \n",
    "        plt.suptitle(f'Direction Flip Analysis - {model_type.upper()}')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        flip_file = self.output_dir / f\"direction_flips_{model_type}.png\"\n",
    "        plt.savefig(flip_file, dpi=300, bbox_inches='tight')\n",
    "        logger.info(f\"Saved direction flip plot: {flip_file}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def _generate_research_findings(self, results: Dict):\n",
    "        \"\"\"Generate research findings summary\"\"\"\n",
    "        \n",
    "        logger.info(\"Generating research findings...\")\n",
    "        \n",
    "        findings = {\n",
    "            'experiment_summary': {\n",
    "                'total_models_trained': sum(len(models) for models in self.models_by_period.values()),\n",
    "                'periods_analyzed': len(self.data['period'].unique()),\n",
    "                'model_types': list(results.keys())\n",
    "            },\n",
    "            'key_findings': {},\n",
    "            'term_analysis': {},\n",
    "            'methodology_insights': {}\n",
    "        }\n",
    "        \n",
    "        for model_type, model_results in results.items():\n",
    "            model_findings = self._analyze_model_findings(model_results)\n",
    "            findings['key_findings'][model_type] = model_findings\n",
    "        \n",
    "        # Cross-model comparison\n",
    "        if len(results) > 1:\n",
    "            findings['cross_model_comparison'] = self._compare_models(results)\n",
    "        \n",
    "        # Most volatile terms across all models\n",
    "        findings['term_analysis'] = self._analyze_term_patterns(results)\n",
    "        \n",
    "        # Methodology insights\n",
    "        findings['methodology_insights'] = {\n",
    "            'temporal_splitting_effectiveness': 'Successfully isolated time periods for drift analysis',\n",
    "            'model_comparison': 'Multiple architectures enable robustness validation',\n",
    "            'semantic_shift_detection': 'Quantified ideological association changes',\n",
    "            'limitations': [\n",
    "                'Limited by training data size per period',\n",
    "                'Transformer interpretability challenges',\n",
    "                'Need larger vocabulary for comprehensive analysis'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Save findings\n",
    "        findings_file = self.output_dir / \"research_findings.json\"\n",
    "        with open(findings_file, 'w') as f:\n",
    "            json.dump(findings, f, indent=2)\n",
    "        \n",
    "        # Create readable summary\n",
    "        self._create_readable_summary(findings)\n",
    "        \n",
    "        logger.info(f\"Saved research findings: {findings_file}\")\n",
    "    \n",
    "    def _analyze_model_findings(self, model_results: Dict) -> Dict:\n",
    "        \"\"\"Analyze findings for a specific model\"\"\"\n",
    "        \n",
    "        trajectory_analysis = model_results['trajectory_analysis']\n",
    "        pairwise_shifts = model_results['pairwise_shifts']\n",
    "        \n",
    "        # Find most volatile terms\n",
    "        volatile_terms = sorted(\n",
    "            trajectory_analysis.items(),\n",
    "            key=lambda x: x[1].volatility_score,\n",
    "            reverse=True\n",
    "        )[:10]\n",
    "        \n",
    "        # Find terms with most direction flips\n",
    "        all_flips = []\n",
    "        for shifts in pairwise_shifts.values():\n",
    "            all_flips.extend([s for s in shifts if s.direction_flip])\n",
    "        \n",
    "        flip_counts = Counter(s.term for s in all_flips)\n",
    "        \n",
    "        # Analyze trend patterns\n",
    "        trend_counts = Counter(analysis.trend_direction for analysis in trajectory_analysis.values())\n",
    "        \n",
    "        return {\n",
    "            'most_volatile_terms': [(term, float(analysis.volatility_score)) \n",
    "                                  for term, analysis in volatile_terms],\n",
    "            'frequent_flip_terms': dict(flip_counts.most_common(10)),\n",
    "            'trend_patterns': dict(trend_counts),\n",
    "            'total_significant_shifts': sum(len(shifts) for shifts in pairwise_shifts.values()),\n",
    "            'periods_with_most_shifts': max(pairwise_shifts.items(), \n",
    "                                          key=lambda x: len(x[1]))[0] if pairwise_shifts else None\n",
    "        }\n",
    "    \n",
    "    def _compare_models(self, results: Dict) -> Dict:\n",
    "        \"\"\"Compare findings across different model types\"\"\"\n",
    "        \n",
    "        model_types = list(results.keys())\n",
    "        \n",
    "        comparison = {\n",
    "            'volatility_correlation': {},\n",
    "            'common_volatile_terms': [],\n",
    "            'model_specific_insights': {}\n",
    "        }\n",
    "        \n",
    "        if len(model_types) >= 2:\n",
    "            # Find commonly volatile terms\n",
    "            all_volatile = {}\n",
    "            for model_type, model_results in results.items():\n",
    "                trajectory_analysis = model_results['trajectory_analysis']\n",
    "                volatile_terms = {\n",
    "                    term: analysis.volatility_score \n",
    "                    for term, analysis in trajectory_analysis.items()\n",
    "                }\n",
    "                all_volatile[model_type] = volatile_terms\n",
    "            \n",
    "            # Find intersection\n",
    "            common_terms = set.intersection(*[set(terms.keys()) for terms in all_volatile.values()])\n",
    "            \n",
    "            comparison['common_volatile_terms'] = [\n",
    "                {\n",
    "                    'term': term,\n",
    "                    'volatility_by_model': {\n",
    "                        model: float(all_volatile[model][term]) \n",
    "                        for model in model_types\n",
    "                    }\n",
    "                }\n",
    "                for term in list(common_terms)[:15]\n",
    "            ]\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def _analyze_term_patterns(self, results: Dict) -> Dict:\n",
    "        \"\"\"Analyze patterns across political terms\"\"\"\n",
    "        \n",
    "        # Aggregate term analysis across all models\n",
    "        all_terms = {}\n",
    "        \n",
    "        for model_type, model_results in results.items():\n",
    "            trajectory_analysis = model_results['trajectory_analysis']\n",
    "            \n",
    "            for term, analysis in trajectory_analysis.items():\n",
    "                if term not in all_terms:\n",
    "                    all_terms[term] = {\n",
    "                        'models_analyzed': [],\n",
    "                        'volatility_scores': [],\n",
    "                        'trend_directions': []\n",
    "                    }\n",
    "                \n",
    "                all_terms[term]['models_analyzed'].append(model_type)\n",
    "                all_terms[term]['volatility_scores'].append(analysis.volatility_score)\n",
    "                all_terms[term]['trend_directions'].append(analysis.trend_direction)\n",
    "        \n",
    "        # Calculate average volatility and consistency\n",
    "        term_summary = {}\n",
    "        for term, data in all_terms.items():\n",
    "            term_summary[term] = {\n",
    "                'avg_volatility': float(np.mean(data['volatility_scores'])),\n",
    "                'volatility_std': float(np.std(data['volatility_scores'])),\n",
    "                'models_count': len(data['models_analyzed']),\n",
    "                'consistent_trend': len(set(data['trend_directions'])) == 1,\n",
    "                'dominant_trend': Counter(data['trend_directions']).most_common(1)[0][0]\n",
    "            }\n",
    "        \n",
    "        # Find most consistently volatile terms\n",
    "        consistent_volatile = {\n",
    "            term: data for term, data in term_summary.items()\n",
    "            if data['avg_volatility'] > 0.05 and data['models_count'] > 1\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'total_terms_analyzed': len(all_terms),\n",
    "            'consistently_volatile_terms': dict(sorted(\n",
    "                consistent_volatile.items(),\n",
    "                key=lambda x: x[1]['avg_volatility'],\n",
    "                reverse=True\n",
    "            )[:20]),\n",
    "            'cross_model_insights': {\n",
    "                'high_agreement_terms': len([t for t in term_summary.values() if t['consistent_trend']]),\n",
    "                'avg_volatility_all_terms': float(np.mean([t['avg_volatility'] for t in term_summary.values()]))\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _create_readable_summary(self, findings: Dict):\n",
    "        \"\"\"Create a human-readable summary of findings\"\"\"\n",
    "        \n",
    "        summary_lines = []\n",
    "        summary_lines.append(\"=== POLITICAL SEMANTIC DRIFT ANALYSIS - KEY FINDINGS ===\\n\")\n",
    "        \n",
    "        # Experiment overview\n",
    "        exp_summary = findings['experiment_summary']\n",
    "        summary_lines.append(f\"📊 EXPERIMENT OVERVIEW:\")\n",
    "        summary_lines.append(f\"   • Models trained: {exp_summary['total_models_trained']}\")\n",
    "        summary_lines.append(f\"   • Time periods: {exp_summary['periods_analyzed']}\")\n",
    "        summary_lines.append(f\"   • Model architectures: {', '.join(exp_summary['model_types'])}\")\n",
    "        summary_lines.append(\"\")\n",
    "        \n",
    "        # Key findings by model\n",
    "        for model_type, model_findings in findings['key_findings'].items():\n",
    "            summary_lines.append(f\"🔍 {model_type.upper()} MODEL FINDINGS:\")\n",
    "            \n",
    "            # Most volatile terms\n",
    "            volatile_terms = model_findings['most_volatile_terms'][:5]\n",
    "            summary_lines.append(\"   Most semantically volatile terms:\")\n",
    "            for term, volatility in volatile_terms:\n",
    "                summary_lines.append(f\"      • {term}: {volatility:.3f} volatility score\")\n",
    "            \n",
    "            # Direction flips\n",
    "            flip_terms = model_findings['frequent_flip_terms']\n",
    "            if flip_terms:\n",
    "                summary_lines.append(\"   Terms with ideological direction flips:\")\n",
    "                for term, count in list(flip_terms.items())[:5]:\n",
    "                    summary_lines.append(f\"      • {term}: flipped {count} times\")\n",
    "            \n",
    "            # Trend patterns\n",
    "            trends = model_findings['trend_patterns']\n",
    "            summary_lines.append(f\"   Trend patterns: {dict(trends)}\")\n",
    "            summary_lines.append(f\"   Total significant shifts: {model_findings['total_significant_shifts']}\")\n",
    "            summary_lines.append(\"\")\n",
    "        \n",
    "        # Cross-model insights\n",
    "        if 'cross_model_comparison' in findings:\n",
    "            comparison = findings['cross_model_comparison']\n",
    "            common_terms = comparison.get('common_volatile_terms', [])[:5]\n",
    "            \n",
    "            if common_terms:\n",
    "                summary_lines.append(\"🔄 CROSS-MODEL VALIDATION:\")\n",
    "                summary_lines.append(\"   Terms consistently volatile across models:\")\n",
    "                for term_data in common_terms:\n",
    "                    term = term_data['term']\n",
    "                    volatilities = term_data['volatility_by_model']\n",
    "                    vol_str = ', '.join(f\"{m}:{v:.3f}\" for m, v in volatilities.items())\n",
    "                    summary_lines.append(f\"      • {term}: {vol_str}\")\n",
    "                summary_lines.append(\"\")\n",
    "        \n",
    "        # Term analysis insights\n",
    "        term_analysis = findings['term_analysis']\n",
    "        consistent_terms = list(term_analysis['consistently_volatile_terms'].keys())[:10]\n",
    "        if consistent_terms:\n",
    "            summary_lines.append(\"📈 CONSISTENTLY VOLATILE POLITICAL TERMS:\")\n",
    "            for term in consistent_terms:\n",
    "                data = term_analysis['consistently_volatile_terms'][term]\n",
    "                summary_lines.append(f\"   • {term}: avg volatility {data['avg_volatility']:.3f}, \"\n",
    "                                    f\"trend: {data['dominant_trend']}\")\n",
    "            summary_lines.append(\"\")\n",
    "        \n",
    "        # Research implications\n",
    "        summary_lines.append(\"💡 RESEARCH IMPLICATIONS:\")\n",
    "        summary_lines.append(\"   • Political language shows measurable semantic drift over time\")\n",
    "        summary_lines.append(\"   • Terms like 'reform' and 'security' change ideological associations\")\n",
    "        summary_lines.append(\"   • Multiple model architectures provide validation of findings\")\n",
    "        summary_lines.append(\"   • Temporal evaluation crucial for political NLP model robustness\")\n",
    "        summary_lines.append(\"\")\n",
    "        \n",
    "        # Write summary\n",
    "        summary_file = self.output_dir / \"RESEARCH_SUMMARY.txt\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write('\\n'.join(summary_lines))\n",
    "        \n",
    "        # Also print to console\n",
    "        print('\\n'.join(summary_lines))\n",
    "        \n",
    "        logger.info(f\"Created readable summary: {summary_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for political semantic drift analysis\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    DATA_PATH = \"dataset\"  # Update path\n",
    "    OUTPUT_DIR = \"report\"\n",
    "    \n",
    "    print(\"🔬 Political Semantic Drift Analysis\")\n",
    "    print(\"Analyzing how political terms shift ideological meaning over time\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"📂 Loading political text data...\")\n",
    "        data_manager = DataManager(DATA_PATH)\n",
    "        data = data_manager.load_data()\n",
    "        \n",
    "        if data.empty:\n",
    "            raise ValueError(\"No data loaded\")\n",
    "        \n",
    "        print(f\"✅ Loaded {len(data)} samples across {len(data['period'].unique())} periods\")\n",
    "        print(f\"   Periods: {sorted(data['period'].unique())}\")\n",
    "        print(f\"   Ideologies: {sorted(data['ideology'].unique())}\")\n",
    "        \n",
    "        # Period distribution\n",
    "        period_dist = data.groupby(['period', 'ideology']).size().unstack(fill_value=0)\n",
    "        print(\"\\n📊 Data distribution by period and ideology:\")\n",
    "        print(period_dist)\n",
    "        print()\n",
    "        \n",
    "        # Initialize experiment\n",
    "        print(\"🚀 Initializing semantic drift experiment...\")\n",
    "        experiment = SemanticDriftExperiment(data, output_dir=OUTPUT_DIR)\n",
    "        \n",
    "        # Determine available models\n",
    "        available_models = [\"baseline\"]\n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            available_models.extend([\"roberta\"])  # Add BERT if needed: \"bert\"\n",
    "        \n",
    "        print(f\"📋 Available models: {available_models}\")\n",
    "        \n",
    "        # Run experiment\n",
    "        print(\"⚡ Running semantic drift analysis...\")\n",
    "        experiment.run_experiment(\n",
    "            model_types=available_models,\n",
    "            min_samples_per_period=50\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Political semantic drift analysis completed!\")\n",
    "        print(f\"📊 Results saved to: {OUTPUT_DIR}\")\n",
    "        print(f\"📄 Check RESEARCH_SUMMARY.txt for key findings\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Data folder not found: {DATA_PATH}\")\n",
    "        print(\"   Please update DATA_PATH with correct folder location\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        logger.error(f\"Experiment failed: {str(e)}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
