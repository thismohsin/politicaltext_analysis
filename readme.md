## Abstract:

Most political NLP models are tested using random splits that ignore how language changes over time. We are examining what happens when you train ideology classifiers on old political texts and test them on newer ones. Our work will use manifestos and parliamentary speeches from several countries to see how classifier performance degrades as the time gap grows. Our study will analyze political texts from party manifestos (2000-2020) and parliamentary speeches (2005-2022) across five countries. We will implement strict temporal splitting where models are trained on data before a cutoff year and tested on data from subsequent years with varying time gaps (1-8 years). We will compare three model architectures: BERT-based classifiers, RoBERTa-based classifiers, and Logistic regression baselines. Performance will be measured using accuracy, F1 score, and class-specific metrics across different temporal gaps. We expect to find substantial accuracy drops over longer periods based on preliminary analysis. The degradation will likely appear roughly linear with time, and transformer models should maintain better performance than traditional approaches but still show significant decline. Feature analysis will be performed using SHAP values to track how specific political terms shift meaning - words like ”reform” and ”security” don’t stay associated with the same ideological camps. We will examine what percentage of discriminative features remain stable versus those that show complete reversal in association. Different ideological classes will be analyzed for varying temporal patterns. This research exposes problems with how we currently evaluate political text classifiers and questions whether models trained on historical data can reliably analyze contemporary politics. The findings will push for better evaluation methods that account for language evolution in political discourse and provide methodological recommendations for more robust evaluation of political NLP systems.

